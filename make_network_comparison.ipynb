{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2886ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from dateutil import tz, parser\n",
    "from datetime import date, time, datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.dates import MonthLocator, DateFormatter, DayLocator, WeekdayLocator\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import matplotlib.transforms as transforms\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "# import contextily as cx\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem()\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import gzip\n",
    "import pickle\n",
    "import pdb\n",
    "from functools import reduce\n",
    "from itertools import product\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import sparse, hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "\n",
    "import pdb\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import uuid\n",
    "import numpy.random as npr\n",
    "import math\n",
    "import itertools\n",
    "import copy\n",
    "import time\n",
    "import scipy.stats as sps\n",
    "from progressbar import ProgressBar\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5aa2ba",
   "metadata": {},
   "source": [
    "# Load data and network created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3553cd16",
   "metadata": {},
   "source": [
    "## Foursquare network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2396528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = s3.glob(\"s3://fsq-phl-data/visits/local_date=*/part*.csv\")\n",
    "fsq_visits = pd.concat([pd.read_csv(s3.open(f)) for f in files]) #read all parts and concatenate\n",
    "\n",
    "# Add the file path column to the DataFrame\n",
    "fsq_visits['file_path'] = pd.Series([f for f in files for i in range(len(pd.read_csv(s3.open(f))))], index=fsq_visits.index)\n",
    "# Create a dictionary to map file paths to date values\n",
    "date_map = {f: f.split(\"=\")[1][:10] for f in files}\n",
    "# Add the date column to the DataFrame\n",
    "fsq_visits['local_date'] = fsq_visits['file_path'].map(date_map)\n",
    "\n",
    "# convert to census tract\n",
    "fsq_visits['home_cbg'] = fsq_visits['home_cbg'].astype(str).str[:-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foursquare POIs in Philadelphia\n",
    "venue_files = s3.glob(\"s3://fsq-full-data/pennu/venues/dt=2022-11-10/part*.csv.gz\")\n",
    "venue_dfs = [pd.read_csv(s3.open(f), compression=\"gzip\", error_bad_lines=False, sep=\"\\t\") for f in venue_files]\n",
    "venue_df = pd.concat(venue_dfs)\n",
    "\n",
    "fsq_phl = venue_df[venue_df['city']=='Philadelphia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c23ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq_visits = fsq_visits[fsq_visits['venue_id'].isin(fsq_phl['venue_id'].tolist())]\n",
    "fsq_visits['dwell_hours'] = fsq_visits['dwell']/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cff4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode time hourly\n",
    "\n",
    "# def duration_time_daily(start, duration):\n",
    "#     return [start + timedelta(days=i) for i in range(duration.days + 1)]\n",
    "\n",
    "def duration_time(start, duration):\n",
    "    return [start + timedelta(hours=i) for i in range(duration.seconds // 3600 + 1)]\n",
    "\n",
    "def explode_time(data):\n",
    "    data['start'] = pd.to_datetime(data['local_date']) + pd.to_timedelta(data['local_hour'], unit='h')\n",
    "    data['end'] = data['start'] + pd.to_timedelta(data['dwell_hours'], unit='h')\n",
    "    data['datetime'] = data.apply(lambda x: duration_time(x['start'], pd.to_timedelta(x['dwell_hours'], unit='h')), axis=1)\n",
    "    data = data.explode('datetime').reset_index(drop=True)\n",
    "    return data.drop_duplicates()\n",
    "\n",
    "exploded_fsq_visits = explode_time(fsq_visits)\n",
    "exploded_fsq_visits.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_fsq_visits['date'] = exploded_fsq_visits['datetime'].apply(lambda x: str(x.date()))\n",
    "exploded_fsq_visits['hour'] = exploded_fsq_visits['datetime'].apply(lambda x: x.hour) # from 0 to 23\n",
    "\n",
    "exploded_fsq_visits.drop(['utc_date','utc_hour','start','end','local_date','local_hour','dwell'],inplace=True,axis=1)\n",
    "exploded_fsq_visits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159658e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby and normalize CBGs to POIs visits\n",
    "fsq_visits_groupby = exploded_fsq_visits.groupby(['venue_id','date','home_cbg','hour']).agg({'full_panel_reweighted_sag_score':sum}).reset_index()\n",
    "fsq_visits_groupby.rename(columns={'full_panel_reweighted_sag_score':'count'},inplace=True)\n",
    "fsq_visits_groupby.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355eddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.read_csv('s3://upenn-seas-wattscovid19lab/paco/acs_vars/safegraph_open_census_data/data/data/cbg_b01.csv', dtype={'census_block_group':str})\n",
    "# Shorten census_block_group to census_tract, convert population to int and group by census_tract\n",
    "population = population.assign(\n",
    "    census_tract = population['census_block_group'].str[:-1],\n",
    "    population = population['B01003e1'].astype(int)).groupby('census_tract')['population'].sum()\n",
    "\n",
    "population = population[population.index.str.startswith('42101')&(population>5)]\n",
    "\n",
    "full_census_tract_list = population.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ca7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 4/1 hour 1 as an example\n",
    "specific_date = '2020-04-01'\n",
    "hour = 1\n",
    "df = fsq_visits_groupby[(fsq_visits_groupby['date']==specific_date) & (fsq_visits_groupby['hour']==hour)]\n",
    "df = df[df.home_cbg.isin(full_census_tract_list)]\n",
    "\n",
    "# make into matrix from CTs to POIs\n",
    "df_matrix = pd.pivot_table(df, values='count', index='home_cbg', columns='venue_id', fill_value=0)\n",
    "# matrix = df_matrix.values\n",
    "\n",
    "# find missing CTs and concat onto the df_matrix\n",
    "unique_census_tracts = df['home_cbg'].unique().tolist()\n",
    "missing_census_tracts = list(set(full_census_tract_list) - set(unique_census_tracts))\n",
    "missing_data = pd.DataFrame(0, index=missing_census_tracts, columns=df_matrix.columns)\n",
    "df_matrix_combined = pd.concat([df_matrix, missing_data])\n",
    "df_matrix_combined = df_matrix_combined.sort_index()\n",
    "df_matrix_combined = df_matrix_combined.fillna(0)\n",
    "matrix = df_matrix_combined.values\n",
    "\n",
    "# M * M^T: make into CTs to CTs\n",
    "matrix_transpose = np.transpose(matrix)\n",
    "result = matrix @ matrix_transpose\n",
    "\n",
    "# Add census tract ids\n",
    "ct_list = df_matrix.index.tolist()\n",
    "result_df = pd.DataFrame(result, columns=full_census_tract_list)\n",
    "result_df['home_CT'] = full_census_tract_list\n",
    "home_CT_column = result_df.pop('home_CT')\n",
    "result_df.insert(0, 'home_CT', home_CT_column)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ab5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export matrix on specific date or date range\n",
    "# def matrix_date(date,optional_date=None):\n",
    "#     if optional_date:\n",
    "#         file_name = date+'_to_'+optional_date\n",
    "#         fsq_visits_groupby['local_date_datetime'] = pd.to_datetime(fsq_visits_groupby['date_str'])\n",
    "#         date = pd.to_datetime(date)\n",
    "#         optional_date = pd.to_datetime(optional_date)\n",
    "#         CBG_POI = fsq_visits_groupby[(fsq_visits_groupby['local_date_datetime'] >= date) & \n",
    "#                                      (fsq_visits_groupby['local_date_datetime'] <= optional_date)]\n",
    "#         CBG_POI = CBG_POI.groupby(['venue_id','home_cbg']).sum().reset_index()\n",
    "#     else:\n",
    "#         file_name = date\n",
    "#         CBG_POI = fsq_visits_groupby[fsq_visits_groupby['date_str']==date]\n",
    "        \n",
    "#     CBG_POI_matrix = pd.pivot_table(CBG_POI, values='count', index='home_cbg', columns='venue_id', fill_value=0)\n",
    "\n",
    "#     # Reindex the matrix to include all CBGs and POIs\n",
    "#     CBG_POI_matrix = CBG_POI_matrix.reindex(index=all_cbgs, columns=all_pois, fill_value=0)\n",
    "# #     print(CBG_POI_matrix.shape)\n",
    "\n",
    "#     CBG_POI_matrix.to_csv('s3://phl-poi-networks/fsq/'+file_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export matrix on specific date or date range\n",
    "# def two_index_date(date):\n",
    "#     file_name = date+'_indexed'\n",
    "#     CBG_POI = fsq_visits_groupby[fsq_visits_groupby['date_str']==date]\n",
    "#     CBG_POI.set_index(['venue_id','home_cbg'],inplace=True)\n",
    "\n",
    "#     CBG_POI.to_csv('s3://phl-poi-networks/fsq/'+file_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select the intersection\n",
    "# for i in range(1,8):\n",
    "#     date ='2020-04-0' + str(i)\n",
    "#     df = fsq_visits_groupby[fsq_visits_groupby['date_str']==date]\n",
    "#     if i == 1:\n",
    "#         old = set(df.home_cbg)\n",
    "#     else:\n",
    "#         new = set(df.home_cbg)\n",
    "#         old = old & new\n",
    "        \n",
    "#     print(len(set(df.home_cbg)))\n",
    "    \n",
    "# intersect_CT = list(old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae15f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CT_to_CT(date):\n",
    "    result_matrix_daily = np.zeros((376,376))\n",
    "    \n",
    "    for hour in range(24):\n",
    "    \n",
    "        df = fsq_visits_groupby[(fsq_visits_groupby['date']==date) & (fsq_visits_groupby['hour']==hour)]\n",
    "        df = df[df.home_cbg.isin(full_census_tract_list)]\n",
    "\n",
    "        # make into matrix from CTs to POIs\n",
    "        df_matrix = pd.pivot_table(df, values='count', index='home_cbg', columns='venue_id', fill_value=0)\n",
    "\n",
    "        # find missing CTs and concat onto the df_matrix\n",
    "        unique_census_tracts = df['home_cbg'].unique().tolist()\n",
    "        missing_census_tracts = list(set(full_census_tract_list) - set(unique_census_tracts))\n",
    "        missing_data = pd.DataFrame(0, index=missing_census_tracts, columns=df_matrix.columns)\n",
    "        df_matrix_combined = pd.concat([df_matrix, missing_data])\n",
    "        df_matrix_combined = df_matrix_combined.sort_index()\n",
    "        df_matrix_combined = df_matrix_combined.fillna(0)\n",
    "        matrix = df_matrix_combined.values\n",
    "\n",
    "        # M * M^T: make into CTs to CTs\n",
    "        matrix_transpose = np.transpose(matrix)\n",
    "        result = matrix @ matrix_transpose\n",
    "        result_matrix_daily += result\n",
    "\n",
    "    n = result_matrix_daily.shape[0] - 1\n",
    "    indices = np.diag_indices(n+1)\n",
    "    result_matrix_daily[indices] /= 2\n",
    "\n",
    "    # Add census tract ids\n",
    "    result_df = pd.DataFrame(result_matrix_daily, columns=full_census_tract_list)\n",
    "    result_df['home_CT'] = full_census_tract_list\n",
    "    home_CT_column = result_df.pop('home_CT')\n",
    "    result_df.insert(0, 'home_CT', home_CT_column)\n",
    "\n",
    "    result_df.to_csv('s3://phl-poi-networks/fsq/'+date+'.csv')\n",
    "    \n",
    "    print('Finished exporting network: ', date)\n",
    "\n",
    "\n",
    "    # Save matrix npz to s3\n",
    "#     s3_client = boto3.client('s3')\n",
    "#     bucket_name = 'phl-poi-networks'\n",
    "#     matrix_bytes = matrix.tobytes()\n",
    "\n",
    "#     s3_client.put_object(Body=matrix_bytes, Bucket=bucket_name, Key=file_name+'.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c7a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = CT_to_CT('2020-04-01')\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daa88f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.strptime(\"04-01-2020\", \"%m-%d-%Y\")\n",
    "end = datetime.strptime(\"04-30-2020\", \"%m-%d-%Y\")\n",
    "date_generated = [start + timedelta(days=x) for x in range(0, (end-start).days)]\n",
    "date_interval = [date.strftime(\"%Y-%m-%d\") for date in date_generated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1dff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in date_interval:\n",
    "    CT_to_CT(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Load the NPZ file\n",
    "# pivot_table = pd.read_csv('s3://phl-poi-networks/fsq/2020-04-01.csv').iloc[:,1:]\n",
    "# non_pivot_df = pd.melt(pivot_table, id_vars=['home_CT'], var_name='column_name', value_name='value')\n",
    "\n",
    "# non_pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export the whole month of April and May\n",
    "# matrix_date('2020-04-01','2020-04-30')\n",
    "# matrix_date('2020-05-01','2020-05-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6027969",
   "metadata": {},
   "source": [
    "## Safegraph network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7305f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to concatenate and groupby to add by hour, indices might be different\n",
    "def add_df(x, y):\n",
    "    return(pd.concat([x, y]).groupby(level=[0,1]).sum())\n",
    "\n",
    "#fix cbgs were incorrectly written when starting with 0, probably because they were kept as ints and not strings ¬¬\n",
    "def fix_cbg(s):\n",
    "    if len(s) == 12:\n",
    "        return(s)\n",
    "    elif len(s) == 11:\n",
    "        return('0'+s)\n",
    "    else:\n",
    "        sys.exit(\"Error, FIPS code is wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'Philadelphia_Camden_Wilmington_PA_NJ_DE_MD'\n",
    "net_file = f+\"_2020-03-01_to_2020-05-02.pkl.gz\"\n",
    "cbgs_file = f+\"_cbg_ids.csv\"\n",
    "pois_file = f+\"_poi_ids.csv\"\n",
    "\n",
    "!aws s3 cp s3://stanford-networks/raw_pickles/$net_file ./../net_file.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb788a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pois = pd.read_csv('s3://stanford-networks/raw_pickles/'+pois_file)['safegraph_place_id']\n",
    "\n",
    "# filter those POIs in Philadelphia\n",
    "safegraph_phl = pd.read_csv(\"s3://upenn-seas-wattscovid19lab/paco/core_places/places-42101.csv\")\n",
    "poi_boolean = [poi in safegraph_phl['safegraph_place_id'].tolist() for poi in pois]\n",
    "sg_poi = pois[poi_boolean]\n",
    "sum(poi_boolean),len(poi_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f8b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbg_fips = pd.read_csv('s3://stanford-networks/raw_pickles/'+cbgs_file, dtype={'cbg_id':'str'})\n",
    "cbg_fips['cbg_id'] = cbg_fips.cbg_id.apply(fix_cbg)\n",
    "census_block_group = cbg_fips.cbg_id\n",
    "census_block_group.name='census_block_group'\n",
    "census_tracts = census_block_group.apply(lambda x: x[:-1]).values\n",
    "\n",
    "# filter those in Philadelphia\n",
    "CT_boolean = np.array([x.startswith('42101') for x in census_tracts], dtype=bool)\n",
    "sg_ct = census_tracts[CT_boolean]\n",
    "sum(CT_boolean),len(CT_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_ct_unique = np.unique(sg_ct).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62fb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sg_ct),len(set(sg_ct))\n",
    "len(np.unique(sg_ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0bdae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we load a list of sparse matrices corresponding to each hour in the time period\n",
    "with gzip.open('./../net_file.pkl.gz', \"rb\") as file:\n",
    "    nets= pickle.load(file)\n",
    "T = len(nets)//24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb4ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "nets[0] #.toarray().shape\n",
    "matrix = nets[0][poi_boolean, :][:,CT_boolean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45725613",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_tanspose(matrix):\n",
    "    matrix_df = pd.DataFrame(matrix.toarray(),columns=sg_ct)\n",
    "    aggregated_df = matrix_df.groupby(matrix_df.columns, axis=1).sum().transpose()\n",
    "    \n",
    "    matrix = aggregated_df.values\n",
    "\n",
    "    # M * M^T: make into CTs to CTs\n",
    "    matrix_transpose = np.transpose(matrix)\n",
    "    result = matrix @ matrix_transpose\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df24d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t represents each of the days we are going to operate on\n",
    "backfill = True\n",
    "first_date = datetime.strptime(\"2020-03-01\",'%Y-%m-%d')\n",
    "for t in tqdm(range(T)):\n",
    "    day_net_name = 's3://phl-poi-networks/stanford/'+ (first_date + timedelta(days=t)).isoformat()[:10] + '.csv'\n",
    "\n",
    "    check_exists = !aws s3 ls $day_net_name\n",
    "    if (not backfill) and len(check_exists)>0:\n",
    "        print(\"Network already exists\")\n",
    "        continue\n",
    " \n",
    "    day_nets = [groupby_tanspose(nets[j][poi_boolean, :][:,CT_boolean]) for j in range(24*t, 24*(t+1))]\n",
    "\n",
    "    m, n = day_nets[0].shape\n",
    "    result = csr_matrix((m, n))\n",
    "\n",
    "    for matrix in day_nets:\n",
    "        result += matrix\n",
    "    \n",
    "#     ct_list = aggregated_df.index.tolist()\n",
    "    result_df = pd.DataFrame(result, columns=sg_ct_unique)\n",
    "    result_df['home_CT'] = sg_ct_unique\n",
    "    home_CT_column = result_df.pop('home_CT')\n",
    "    result_df.insert(0, 'home_CT', home_CT_column)\n",
    "#     print(result_df.shape) \n",
    "#     break\n",
    "    \n",
    "    result_df.to_csv(day_net_name)\n",
    "    print('Finished exporting network: ', day_net_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a9a1ee",
   "metadata": {},
   "source": [
    "## Load stored networks\n",
    "\n",
    "/multiscale_epidemic/stanford_nets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '04-01'\n",
    "fsq_path = 's3://phl-poi-networks/fsq/2020-'+date+'.csv'\n",
    "sg_path = 's3://phl-poi-networks/stanford/2020-'+date+'.csv'\n",
    "\n",
    "fsq_network = pd.read_csv(fsq_path).iloc[:,1:]\n",
    "sg_network = pd.read_csv(sg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd6bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq_network.shape, sg_network.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq_network.iloc[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1649bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_network.iloc[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87739bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq_ct = fsq_network['home_cbg'] # integer\n",
    "fsq_poi = fsq_network.columns[1:]\n",
    "\n",
    "sg_ct = sg_network['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdce700",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'Philadelphia_Camden_Wilmington_PA_NJ_DE_MD'\n",
    "net_file = f+\"_2020-03-01_to_2020-05-02.pkl.gz\"\n",
    "cbgs_file = f+\"_cbg_ids.csv\"\n",
    "pois_file = f+\"_poi_ids.csv\"\n",
    "\n",
    "!aws s3 cp s3://stanford-networks/raw_pickles/$net_file ./../net_file.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdfd37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pois = pd.read_csv('s3://stanford-networks/raw_pickles/'+pois_file)['safegraph_place_id']\n",
    "\n",
    "# filter those POIs in Philadelphia\n",
    "safegraph_phl = pd.read_csv(\"s3://upenn-seas-wattscovid19lab/paco/core_places/places-42101.csv\")\n",
    "poi_boolean = [poi in safegraph_phl['safegraph_place_id'].tolist() for poi in pois]\n",
    "sg_poi = pois[poi_boolean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd2ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq_not_in_sg = 0\n",
    "fsq_not_in_sg_lst = []\n",
    "for i in range(len(fsq_ct)):\n",
    "    if not fsq_ct.iloc[i] in sg_ct.tolist():\n",
    "        fsq_not_in_sg += 1\n",
    "        fsq_not_in_sg_lst.append(fsq_ct.iloc[i])\n",
    "#         print(fsq_ct.iloc[i])\n",
    "print('In Foursquare, there are %i census tracts that are not in Safegraph CT list'%(fsq_not_in_sg))\n",
    "\n",
    "sg_not_in_fsq = 0\n",
    "sg_not_in_fsq_lst = []\n",
    "for ct in sg_ct:\n",
    "    if not ct in fsq_ct.tolist():\n",
    "        sg_not_in_fsq += 1\n",
    "        sg_not_in_fsq_lst.append(ct)\n",
    "#         print(ct)\n",
    "print('In Safegraph, there are %i census tracts that are not in Foursquare CT list'%(sg_not_in_fsq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae27b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee0c4cb9",
   "metadata": {},
   "source": [
    "# Matrix calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edefd45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq_sparse_matrix, sg_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a0b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq_sparse_matrix.nnz, sg_sparse_matrix.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(fsq_nonzero), np.count_nonzero(sg_nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrames to sparse matrices (assuming your DataFrames contain numeric values)\n",
    "fsq_sparse_matrix = csr_matrix(fsq_network.iloc[:,1:].values)\n",
    "sg_sparse_matrix = csr_matrix(sg_network.iloc[:,1:].values)\n",
    "\n",
    "# Calculate the sparsity of each network considering values larger than 5\n",
    "fsq_nonzero = fsq_sparse_matrix.data > 5\n",
    "fsq_sparsity = 1.0 - (np.count_nonzero(fsq_nonzero) / np.prod(fsq_sparse_matrix.shape))\n",
    "\n",
    "sg_nonzero = sg_sparse_matrix.data > 5\n",
    "sg_sparsity = 1.0 - (np.count_nonzero(sg_nonzero) / np.prod(sg_sparse_matrix.shape))\n",
    "\n",
    "# Calculate the density of each network considering values larger than 5\n",
    "fsq_density = np.count_nonzero(fsq_nonzero) / np.prod(fsq_sparse_matrix.shape)\n",
    "sg_density = np.count_nonzero(sg_nonzero) / np.prod(sg_sparse_matrix.shape)\n",
    "\n",
    "print(\"Sparsity Comparison:\")\n",
    "print(\"fsq Sparsity:\", fsq_sparsity)\n",
    "print(\"Safegraph Sparsity:\", sg_sparsity)\n",
    "\n",
    "print(\"Density Comparison:\")\n",
    "print(\"fsq Density:\", fsq_density)\n",
    "print(\"Safegraph Density:\", sg_density)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "# Calculate the degree distribution of each network (in-degree and out-degree)\n",
    "fsq_in_degree = np.asarray(fsq_sparse_matrix.sum(axis=0)).flatten()\n",
    "fsq_out_degree = np.asarray(fsq_sparse_matrix.sum(axis=1)).flatten()\n",
    "\n",
    "sg_in_degree = np.asarray(sg_sparse_matrix.sum(axis=0)).flatten()\n",
    "sg_out_degree = np.asarray(sg_sparse_matrix.sum(axis=1)).flatten()\n",
    "\n",
    "non_zero_fsq_in_degree = fsq_in_degree[fsq_in_degree > 5]\n",
    "print('Foursquare in-degree (POIs) density:',len(non_zero_fsq_in_degree),'/',len(fsq_in_degree),'=',len(non_zero_fsq_in_degree)/len(fsq_in_degree))\n",
    "\n",
    "non_zero_sg_in_degree = sg_in_degree[sg_in_degree > 5]\n",
    "print('Safegraph in-degree (POIs) density:',len(non_zero_sg_in_degree),'/',len(sg_in_degree),'=',len(non_zero_sg_in_degree)/len(sg_in_degree))\n",
    "\n",
    "non_zero_fsq_out_degree =fsq_out_degree[fsq_out_degree >5]\n",
    "print('Foursquare out-degree (CBGs) density:',len(non_zero_fsq_out_degree),'/',len(fsq_out_degree),'=',len(non_zero_fsq_out_degree)/len(fsq_out_degree))\n",
    "\n",
    "non_zero_sg_out_degree = sg_out_degree[sg_out_degree >5]\n",
    "print('Safegraph out-degree (CBGs) density:',len(non_zero_sg_out_degree),'/',len(sg_out_degree),'=',len(non_zero_sg_out_degree)/len(sg_out_degree))\n",
    "\n",
    "\n",
    "# Compare the degree distributions using statistical measures or visualization\n",
    "# For example, you can compare the means of the in-degree distributions\n",
    "fsq_mean_in_degree1 = np.mean(fsq_in_degree)\n",
    "sg_mean_in_degree2 = np.mean(sg_in_degree)\n",
    "\n",
    "# Alternatively, you can visualize the degree distributions using histograms\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axs[0].set_xlim((0, 2000))\n",
    "axs[0].set_xlabel('Total Visits of POIs in Foursquare')\n",
    "axs[0].set_ylabel('count of POIs')\n",
    "axs[0].legend()\n",
    "bin_edges = np.linspace(0, 2000, num=11)  # 11 edges for 10 bins\n",
    "axs[0].hist(non_zero_fsq_in_degree, bins=bin_edges, alpha=0.5, label='Foursquare')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_xlim((0, 200))\n",
    "axs[1].set_xlabel('Total Visits of POIs in Safegraph')\n",
    "axs[1].set_ylabel('count of POIs')\n",
    "axs[1].legend()\n",
    "bin_edges = np.linspace(0, 200, num=11)  # 11 edges for 10 bins\n",
    "axs[1].hist(non_zero_sg_in_degree, bins=bin_edges, alpha=0.5, label='Safegraph', color='orange')\n",
    "axs[1].legend()\n",
    "\n",
    "fig.suptitle('POIs visits distribution on 4/1')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd10346",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in fsq_ct[fsq_out_degree <= 5].tolist():\n",
    "    if i in fsq_not_in_sg_lst:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fsq_not_in_sg_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21873b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "sg_network.iloc[:,i][sg_network.iloc[:,i] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63210ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq_network.iloc[:,i][fsq_network.iloc[:,i] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb456cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fsq_out_degree)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# axs[0].set_xlim((0,800))\n",
    "# axs[0].hist(non_zero_fsq_out_degree, alpha=0.5, label='fsq')\n",
    "# axs[0].set_xlabel('Visits in April 1st')\n",
    "# axs[0].set_ylabel('number of CBGs')\n",
    "# axs[0].legend()\n",
    "\n",
    "axs[0].set_xlim((0, 10000))\n",
    "axs[0].set_xlabel('Total Visits of CTs in Foursquare')\n",
    "axs[0].set_ylabel('count of CTs')\n",
    "axs[0].legend()\n",
    "bin_edges = np.linspace(0, 10000, num=11)  # 11 edges for 10 bins\n",
    "axs[0].hist(non_zero_fsq_out_degree, bins=bin_edges, alpha=0.5, label='Foursquare')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].hist(non_zero_sg_out_degree, alpha=0.5, label='Safegraph', color='orange')\n",
    "axs[1].set_xlabel('Total Visits of CTs in Safegraph')\n",
    "axs[1].set_ylabel('count of CTs')\n",
    "axs[1].legend()\n",
    "\n",
    "# axs[1].set_xlim((0, 10000))\n",
    "# axs[1].set_xlabel('Total Visits of CTs in Foursquare')\n",
    "# axs[1].set_ylabel('count of CTs')\n",
    "# axs[1].legend()\n",
    "# bin_edges = np.linspace(0, 10000, num=11)  # 11 edges for 10 bins\n",
    "# axs[1].hist(non_zero_fsq_out_degree, bins=bin_edges, alpha=0.5, label='Foursquare')\n",
    "# axs[1].legend()\n",
    "\n",
    "fig.suptitle('CTs outgoing visits distribution on 4/1')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "# Create bipartite graphs from the non-square matrices\n",
    "fsq_bipartite = nx.bipartite.from_biadjacency_matrix(fsq_sparse_matrix)\n",
    "sg_bipartite = nx.bipartite.from_biadjacency_matrix(sg_sparse_matrix)\n",
    "\n",
    "# Specify the set of nodes to project onto\n",
    "fsq_nodes = {n for n, d in fsq_bipartite.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "sg_nodes = {n for n, d in sg_bipartite.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "\n",
    "# Project the bipartite graphs onto the specified nodes\n",
    "fsq_projected = nx.bipartite.projected_graph(fsq_bipartite, fsq_nodes)\n",
    "sg_projected = nx.bipartite.projected_graph(sg_bipartite, sg_nodes)\n",
    "\n",
    "# Compute the communities using the Louvain method\n",
    "fsq_communities = community.greedy_modularity_communities(fsq_projected)\n",
    "sg_communities = community.greedy_modularity_communities(sg_projected)\n",
    "\n",
    "fsq_communities = [set(c) for c in fsq_communities]\n",
    "sg_communities = [set(c) for c in sg_communities]\n",
    "\n",
    "# Compute the modularity of the communities\n",
    "fsq_modularity = community.quality.modularity(fsq_projected, fsq_communities)\n",
    "sg_modularity = community.quality.modularity(sg_projected, sg_communities)\n",
    "\n",
    "# Compare the modularity values\n",
    "if fsq_modularity > sg_modularity:\n",
    "    print(\"The modularity of the fsq_network is higher.\")\n",
    "elif fsq_modularity < sg_modularity:\n",
    "    print(\"The modularity of the sg_network is higher.\")\n",
    "else:\n",
    "    print(\"The modularity values are equal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a32fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq_modularity, sg_modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "# Create bipartite graphs from the non-square matrices\n",
    "fsq_bipartite = nx.from_scipy_sparse_matrix(fsq_sparse_matrix)\n",
    "sg_bipartite = nx.from_scipy_sparse_matrix(sg_sparse_matrix)\n",
    "\n",
    "# Get the set of nodes of the desired type (rows) for each bipartite graph\n",
    "fsq_nodes = {n for n, d in fsq_bipartite.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "sg_nodes = {n for n, d in sg_bipartite.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "\n",
    "# Project the bipartite graphs onto the specified nodes\n",
    "fsq_projected = bipartite.projected_graph(fsq_bipartite, fsq_nodes)\n",
    "sg_projected = bipartite.projected_graph(sg_bipartite, sg_nodes)\n",
    "\n",
    "# Compute the communities using the Louvain method\n",
    "fsq_communities, _ = bipartite.modularity(fsq_projected, fsq_nodes)\n",
    "sg_communities, _ = bipartite.modularity(sg_projected, sg_nodes)\n",
    "\n",
    "# Calculate the modularity for each network\n",
    "fsq_modularity = bipartite.modularity(fsq_projected, fsq_communities)\n",
    "sg_modularity = bipartite.modularity(sg_projected, sg_communities)\n",
    "\n",
    "# Compare the modularity values\n",
    "if fsq_modularity > sg_modularity:\n",
    "    print(\"The modularity of the fsq_network is higher.\")\n",
    "elif fsq_modularity < sg_modularity:\n",
    "    print(\"The modularity of the sg_network is higher.\")\n",
    "else:\n",
    "    print(\"The modularity values are equal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab537973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Comparisons:\n",
    "\n",
    "# 1. Clustering Coefficient:\n",
    "fsq_avg_clustering = nx.average_clustering(nx.from_scipy_sparse_matrix(fsq_sparse_matrix))\n",
    "sq_avg_clustering = nx.average_clustering(nx.from_scipy_sparse_matrix(sq_sparse_matrix))\n",
    "\n",
    "# 2. Assortativity:\n",
    "fsq_assortativity = nx.degree_assortativity_coefficient(nx.from_scipy_sparse_matrix(fsq_sparse_matrix))\n",
    "sq_assortativity = nx.degree_assortativity_coefficient(nx.from_scipy_sparse_matrix(sq_sparse_matrix))\n",
    "\n",
    "# 3. Network Distance:\n",
    "fsq_avg_shortest_path = nx.average_shortest_path_length(nx.from_scipy_sparse_matrix(fsq_sparse_matrix))\n",
    "sq_avg_shortest_path = nx.average_shortest_path_length(nx.from_scipy_sparse_matrix(sq_sparse_matrix))\n",
    "\n",
    "# 4. Centrality Measures:\n",
    "fsq_degree_centrality = nx.degree_centrality(nx.from_scipy_sparse_matrix(fsq_sparse_matrix))\n",
    "sq_degree_centrality = nx.degree_centrality(nx.from_scipy_sparse_matrix(sq_sparse_matrix))\n",
    "\n",
    "# 5. Network Motifs:\n",
    "fsq_motifs = nx.algorithms.motifs(nx.from_scipy_sparse_matrix(fsq_sparse_matrix))\n",
    "sq_motifs = nx.algorithms.motifs(nx.from_scipy_sparse_matrix(sq_sparse_matrix))\n",
    "\n",
    "# 6. Community Structure:\n",
    "fsq_communities = nx.algorithms.community.greedy_modularity_communities(nx.from_scipy_sparse_matrix(fsq_sparse_matrix))\n",
    "sq_communities = nx.algorithms.community.greedy_modularity_communities(nx.from_scipy_sparse_matrix(sq_sparse_matrix))\n",
    "\n",
    "# 7. Network Robustness:\n",
    "fsq_giant_component_size = nx.algorithms.components.number_connected_components(nx.from_scipy_sparse_matrix(fsq_sparse_matrix))\n",
    "sq_giant_component_size = nx.algorithms.components.number_connected_components(nx.from_scipy_sparse_matrix(sq_sparse_matrix))\n",
    "\n",
    "# 8. Network Evolution:\n",
    "# Perform temporal analysis to compare networks across different time points or intervals\n",
    "\n",
    "# Print the results of advanced comparisons:\n",
    "\n",
    "print(\"Advanced Comparisons:\")\n",
    "print(\"Clustering Coefficient Comparison:\")\n",
    "print(\"Foursquare Average Clustering Coefficient:\", fsq_avg_clustering)\n",
    "print(\"Safegraph Average Clustering Coefficient:\", sq_avg_clustering)\n",
    "\n",
    "print(\"Assortativity Comparison:\")\n",
    "print(\"Foursquare Assortativity Coefficient:\", fsq_assortativity)\n",
    "print(\"Safegraph Assortativity Coefficient:\", sq_assortativity)\n",
    "\n",
    "print(\"Network Distance Comparison:\")\n",
    "print(\"Foursquare Average Shortest Path Length:\", fsq_avg_shortest_path)\n",
    "print(\"Safegraph Average Shortest Path Length:\", sq_avg_shortest_path)\n",
    "\n",
    "print(\"Centrality Measures Comparison:\")\n",
    "print(\"Foursquare Degree Centrality:\", fsq_degree_centrality)\n",
    "print(\"Safegraph Degree Centrality:\", sq_degree_centrality)\n",
    "\n",
    "print(\"Network Motifs Comparison:\")\n",
    "print(\"Foursquare Network Motifs:\", fsq_motifs)\n",
    "print(\"Safegraph Network Motifs:\", sq_motifs)\n",
    "\n",
    "print(\"Community Structure Comparison:\")\n",
    "print(\"Foursquare Communities:\", fsq_communities)\n",
    "print(\"Safegraph Communities:\", sq_communities)\n",
    "\n",
    "print(\"Network Robustness Comparison:\")\n",
    "print(\"Foursquare Giant Component Size:\", fsq_giant_component_size)\n",
    "print(\"Safegraph Giant Component Size:\", sq_giant_component_size)\n",
    "\n",
    "# Continue with other comparisons and calculations as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fea31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Perform other desired comparisons and calculations\n",
    "# such as modularity, edge variance, central nodes, etc.\n",
    "\n",
    "# # Example: Create networkx graphs for visualization\n",
    "# fsq_graph = nx.from_scipy_sparse_matrix(fsq_sparse_matrix)\n",
    "# sg_graph = nx.from_scipy_sparse_matrix(sg_sparse_matrix)\n",
    "\n",
    "# # Example: Calculate modularity using networkx\n",
    "# fsq_modularity = nx.algorithms.community.modularity(fsq_graph, fsq_graph.nodes())\n",
    "# sg_modularity = nx.algorithms.community.modularity(sg_graph, sg_graph.nodes())\n",
    "\n",
    "# # Example: Compare modularity\n",
    "# print(\"Modularity Comparison:\")\n",
    "# print(\"fsq Modularity:\", fsq_modularity)\n",
    "# print(\"Safegraph Modularity:\", sg_modularity)\n",
    "\n",
    "# Example: Compare sparsity and density\n",
    "\n",
    "\n",
    "# Continue with other comparisons and calculations as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Assuming you have the necessary data and calculations from previous code sections\n",
    "\n",
    "# Example 1: Degree Distribution Comparison\n",
    "plt.hist(fsq_out_degree, bins=10, alpha=0.5, label='Foursquare')\n",
    "plt.hist(sq_out_degree, bins=10, alpha=0.5, label='Safegraph')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Degree Distribution Comparison')\n",
    "plt.show()\n",
    "\n",
    "# Example 2: Network Structure Comparison\n",
    "fsq_graph = nx.from_scipy_sparse_matrix(fsq_sparse_matrix)\n",
    "sq_graph = nx.from_scipy_sparse_matrix(sq_sparse_matrix)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "nx.draw(fsq_graph, with_labels=True)\n",
    "plt.title('Foursquare Network')\n",
    "plt.subplot(122)\n",
    "nx.draw(sq_graph, with_labels=True)\n",
    "plt.title('Safegraph Network')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Example 3: Community Structure Comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "nx.draw(fsq_graph, with_labels=True, node_color='lightblue')\n",
    "plt.title('Foursquare Network')\n",
    "plt.subplot(122)\n",
    "nx.draw(sq_graph, with_labels=True, node_color='lightgreen')\n",
    "plt.title('Safegraph Network')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Example 4: Centrality Comparison\n",
    "plt.scatter(range(len(fsq_degree_centrality)), list(fsq_degree_centrality.values()), label='Foursquare')\n",
    "plt.scatter(range(len(sq_degree_centrality)), list(sq_degree_centrality.values()), label='Safegraph')\n",
    "plt.xlabel('Node Index')\n",
    "plt.ylabel('Centrality')\n",
    "plt.legend()\n",
    "plt.title('Centrality Comparison')\n",
    "plt.show()\n",
    "\n",
    "# Example 5: Network Evolution Comparison (using hypothetical data)\n",
    "time_points = [t1, t2, t3]  # Replace with your actual time points\n",
    "densities_fsq = [0.8, 0.7, 0.6]  # Replace with your actual density values for Foursquare\n",
    "densities_sq = [0.9, 0.8, 0.7]  # Replace with your actual density values for Safegraph\n",
    "plt.plot(time_points, densities_fsq, marker='o', label='Foursquare')\n",
    "plt.plot(time_points, densities_sq, marker='o', label='Safegraph')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title('Network Density Evolution')\n",
    "plt.show()\n",
    "\n",
    "# Example 6: Heatmap Comparison (using hypothetical edge weight data)\n",
    "edge_weights_fsq = np.random.rand(fsq_sparse_matrix.nnz)\n",
    "edge_weights_sq = np.random.rand(sq_sparse_matrix.nnz)\n",
    "heatmap_data = [edge_weights_fsq, edge_weights_sq]\n",
    "plt.imshow(heatmap_data, cmap='hot', aspect='auto')\n",
    "plt.colorbar(label='Edge Weight')\n",
    "plt.xticks([])  # Assuming you don't want x-axis tick labels\n",
    "plt.ylabel('Edge Index')\n",
    "plt.title('Edge Weight Heatmap Comparison')\n",
    "plt.show()\n",
    "\n",
    "# Continue with other plot comparisons as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565419f",
   "metadata": {},
   "source": [
    "# epidemic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e11f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.read_csv(\n",
    "    's3://upenn-seas-wattscovid19lab/paco/acs_vars/safegraph_open_census_data/data/data/cbg_b01.csv',\n",
    "    dtype = {'census_block_group':str})\n",
    "population = population.loc[population.census_block_group.apply(lambda x: x[:5]=='42101')].set_index('census_block_group').B01003e1\n",
    "population.name = 'population'\n",
    "population.index = population.index.map(lambda x: x[:-1])\n",
    "population.index.name = 'census_tract'\n",
    "population = population.astype(int)\n",
    "population = population.groupby('census_tract').sum()\n",
    "population = population.to_frame().reset_index()\n",
    "# population['census_block_group'] = population['census_block_group'].astype(int)\n",
    "total_population = population['population'].sum()\n",
    "population['census_tract'] = population['census_tract'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b847d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "population[population['population']>8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_geoid = population.set_index('census_tract')\n",
    "pop_geoid = pop_geoid.rename_axis('origin_geoid')\n",
    "pop_geoid.index = pop_geoid.index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e8c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_date = datetime.strptime('2020-04-01', '%Y-%m-%d')\n",
    "\n",
    "def day_interval(date_start, num_days):\n",
    "    day_list = [date_start+timedelta(days=t) for t in range(num_days)]\n",
    "    return(day_list)\n",
    "\n",
    "date_interval = day_interval(init_date, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dict = {}\n",
    "for d in date_interval:\n",
    "    date = str(d)[:10]\n",
    "    fsq_path = 's3://phl-poi-networks/fsq/'+date+'_indexed.csv'\n",
    "    net = pd.read_csv(fsq_path)[['venue_id','home_cbg','count']]\n",
    "    net.rename(columns={'home_cbg':'origin_geoid','venue_id':'dest_geoid'},inplace=True)\n",
    "    net.set_index(['origin_geoid','dest_geoid'],inplace=True)\n",
    "    date_obj = datetime.strptime(date, '%Y-%m-%d')\n",
    "    net_dict[date_obj] = net\n",
    "    print(date_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'beta' : 2000,\n",
    "    'kappa' : 0.22,\n",
    "    'gamma' : 0.14,\n",
    "    'tau' : 0.08}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def best_guess_x0(init_date, params, cases_df, pop_geoid, which='middle'):\n",
    "#     sum_caseload = np.sum(cases_df[cases_df.index <= init_date])\n",
    "#     window = [init_date - timedelta(t) for t in range(12)]\n",
    "#     if which=='middle':\n",
    "#         #we find good guesses for E and I based on tau, real and kappa\n",
    "#         #new cases at the beggining is approx kappa*E*tau\n",
    "#         E = cases_df[init_date]\\\n",
    "#              /(params['kappa']*params['tau'])\n",
    "#         #look 10 days into the past for new cases, and estimate the \n",
    "#         #number still in I using gamma\n",
    "#         I = [cases*(1-params['gamma'])**(t) for t, cases\n",
    "#                               in enumerate(cases_df.reindex(window))]\n",
    "#         I = pd.Series(I).sum()/params['tau'] #to ignore nan\n",
    "#     elif which == 'low':\n",
    "#         E = (1-params['kappa'])*real[init_date]\\\n",
    "#              /(params['kappa']*params['tau'])\n",
    "#         E = E - np.sqrt(E)\n",
    "\n",
    "#         I = [cases*(1-params['gamma'])**(t+1) for t, cases\n",
    "#                               in enumerate(cases_df.reindex(window))]\n",
    "#         I = [x - np.sqrt(x) for x in I]\n",
    "#         I = pd.Series(I).sum()/params['tau'] #to ignore nan\n",
    "\n",
    "#     elif which == 'high':\n",
    "#         E = (1-params['kappa'])*real[init_date-timedelta(days=1)]\\\n",
    "#              /(params['kappa']*params['tau'])\n",
    "#         E = E + np.sqrt(E)\n",
    "\n",
    "#         I = [cases*(1-params['gamma'])**(t+1) for t, cases\n",
    "#                               in enumerate(cases_df.reindex(window))]\n",
    "#         I = [x + np.sqrt(x) for x in I]\n",
    "#         I = pd.Series(I).sum()/params['tau'] #to ignore nan\n",
    "\n",
    "#     #COMPUTE S RESIDUALLY\n",
    "#     S = np.sum(pop_geoid) - E - sum_caseload/params['tau']\n",
    "#     return(int(S),int(E),int(I))\n",
    "\n",
    "# params['S'], params['E'], params['I'] = best_guess_x0(\n",
    "#         init_date, params, cases_df, pop_geoid, which = 'low')\n",
    "\n",
    "params['S'], params['E'], params['I'] = 100000, 10000, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd418fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc1cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb597d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_states_list(initial_cond, i, pop_geoid):\n",
    "    mu = initial_cond[i, :]\n",
    "    n = len(pop_geoid)\n",
    "\n",
    "    states = pd.DataFrame(index=pop_geoid.index)\n",
    "    states['N'] = pop_geoid.copy() #total population\n",
    "\n",
    "    states['S'] = np.maximum(mu[:n],0) \n",
    "    states['S/N'] = states['S']/states['N']\n",
    "\n",
    "    states['E'] = np.maximum(mu[2*n:3*n],0)\n",
    "    states['I'] = np.maximum(mu[4*n:5*n],0)\n",
    "\n",
    "    states['I/N'] = states['I']/states['N']\n",
    "    return(states)\n",
    "\n",
    "def initialize_states(pop_geoid, params,model='SEIR'):\n",
    "    if 'S' not in params.keys():\n",
    "        sys.exit(\"Initial conditions must be in params to use this method\")\n",
    "    states = pd.DataFrame(index=pop_geoid.index)\n",
    "    states['N'] = pop_geoid.copy() #total population\n",
    "    probs = states['N']/np.sum(states['N'])\n",
    "\n",
    "    N_minus_S = np.sum(states['N']) - params['S']\n",
    "    N_minus_S  = npr.multinomial(int(N_minus_S), pvals = probs) \n",
    "\n",
    "    states['S'] = states['N'] - N_minus_S \n",
    "    states['S'] = states[['N','S']].min(axis=1)\n",
    "    states['S/N'] = states['S']/states['N']\n",
    "\n",
    "    states['E'] = npr.multinomial(int(params['E']), pvals = probs) \n",
    "    states['E'] = states[['N','E']].min(axis=1)\n",
    "\n",
    "    states['I'] = npr.multinomial(int(params['I']), pvals = probs) \n",
    "    states['I'] = states[['N','I']].min(axis=1)\n",
    "\n",
    "    if model == 'SEIIR':\n",
    "        states['I_s'] = (states['I']*params['rho']).round()\n",
    "        states['I_a'] = (states['I']*(1-params['rho'])).round()\n",
    "        states['I_s/N'] = states['I_s']/states['N']\n",
    "        states['I_a/N'] = states['I_a']/states['N']\n",
    "        states['I'] = None\n",
    "    elif model == 'SEIR':\n",
    "        states['I/N'] = states['I']/states['N']\n",
    "    else:\n",
    "        sys.exit('model not programmed')\n",
    "    return(states)\n",
    "\n",
    "def which_date(date, weekly_dates):\n",
    "    \"\"\"\n",
    "    Obtain valid dates of patterns files, the date represents the Monday\n",
    "    of the relevant week.\n",
    "\n",
    "    parameters:\n",
    "        date datetime.datetime object\n",
    "        patterns_path to weekly patterns data files\n",
    "    \"\"\"\n",
    "    #compare date with patterns_dates and find the last one that is less\n",
    "    date_inds = [i for i, x in enumerate(weekly_dates) if date >= x]\n",
    "    if len(date_inds) == 0: return(weekly_dates[0])\n",
    "    return( weekly_dates[ max(date_inds)])\n",
    "\n",
    "def update_states(states, params, net, model='SEIR', log_p=False):\n",
    "    \"\"\"\n",
    "    Takes the states of the model in a given day and simulates the \n",
    "    compartmental transitions. The only non-static parameter, is the \n",
    "    contact network. \n",
    "    params:\n",
    "        states: dict\n",
    "            current compartment counts for every subpopulation.\n",
    "        alpha: float\n",
    "            discount factor for contact with an asymptomatic infective.\n",
    "        beta: float\n",
    "            rate of infection the probability of a susceptible becoming \n",
    "            infected from exposure to one infective/sq_foot. \n",
    "        gamma: float\n",
    "            rate at which infectives recover.\n",
    "        kappa: float\n",
    "            rate at which an exposed becomes infective.\n",
    "        rho: float\n",
    "            probability of exposed becoming symptomatic.\n",
    "        net: pandas DataFrame\n",
    "            network estimating the number of contacts with other CBGs\n",
    "            in a given day.\n",
    "        net: dict\n",
    "            contact network with double index (origin_geoid, dest_geoid) \n",
    "        model: str\n",
    "            the type of compartmental model\n",
    "        out_trans: bool\n",
    "            if trans, we return the transitions\n",
    "            for likelihood computation\n",
    "    \"\"\"\n",
    "    #compute effective infection rates\n",
    "    net = net.join(states['S/N'], on='origin_geoid')\n",
    "    \n",
    "    if model == 'SEIIR':\n",
    "        net = net.join(\n",
    "            states[['I_a/N', 'I_s/N']],\n",
    "            on='destination_geoid')\n",
    "        effective_rates = compute_rate(\n",
    "            net,\n",
    "            states=states,\n",
    "            alpha=params['alpha'],\n",
    "            beta=params['beta'])\n",
    "    elif model == 'SEIR':\n",
    "        print('Problem occurs=========================')\n",
    "#         print(net.index)\n",
    "        net = net.join(states['I/N'], on='destination_geoid')  # why on destination_geoid??\n",
    "        effective_rates = compute_rate_SEIR(\n",
    "            net,\n",
    "            beta=params['beta'])\n",
    "    #sample state transitions\n",
    "    if log_p:\n",
    "        trans, p = state_transitions(states, effective_rates, params, model, log_p)\n",
    "        apply_transitions(states, trans, params, model)\n",
    "        return(states, trans['E_to_I'], p)\n",
    "    else:\n",
    "        trans = state_transitions(states, effective_rates, params, model)\n",
    "        apply_transitions(states, trans, params, model)\n",
    "        return((states, trans['E_to_I']))\n",
    "    \n",
    "def run_model(params,\n",
    "              net_dict,\n",
    "              pop_geoid,\n",
    "              GEOID_type,\n",
    "              date_interval,\n",
    "              model='SEIR',\n",
    "              num_sims=36,\n",
    "              initial_cond=None):\n",
    "    caseload_df = pd.DataFrame({'date':date_interval})\n",
    "\n",
    "    pbar = ProgressBar()\n",
    "    for i in pbar(range(num_sims)):\n",
    "        if initial_cond is not None:\n",
    "            states = initialize_states_list(\n",
    "                initial_cond,\n",
    "                i,\n",
    "                pop_geoid)\n",
    "        else:\n",
    "            states = initialize_states(\n",
    "                pop_geoid,\n",
    "                params=params)\n",
    "        print('Initialized states:')\n",
    "        print(states)\n",
    "        caseload = []\n",
    "        patterns_date = ''\n",
    "        for date in date_interval:\n",
    "            #print(\"Simulating for date {}\".format(date.isoformat()[:10]))\n",
    "            if patterns_date != which_date(date, list(net_dict.keys())):\n",
    "                #print(\"--Changing contact network\")\n",
    "                patterns_date = which_date(date, list(net_dict.keys()))\n",
    "                net=net_dict[patterns_date]\n",
    "\n",
    "            states, new_cases = update_states(\n",
    "                states=states,\n",
    "                params=params,\n",
    "                net=net)\n",
    "            caseload.append(new_cases.sum())\n",
    "\n",
    "        caseload_df[i]=caseload\n",
    "    caseload_df.set_index('date',inplace=True)\n",
    "    return(caseload_df)\n",
    "\n",
    "stime = time.time()\n",
    "df = run_model(\n",
    "    params,\n",
    "    net_dict,\n",
    "    pop_geoid,\n",
    "    'CT',\n",
    "    date_interval)\n",
    "print(\"Elapsed time: {}\".format(time.time()-stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = time.time()\n",
    "df = run_model(\n",
    "    params,\n",
    "    net_dict,\n",
    "    pop_geoid,\n",
    "    'CT',\n",
    "    date_interval)\n",
    "print(\"Elapsed time: {}\".format(time.time()-stime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca46d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
